{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c39fbffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from Modules import ScaledDotProductAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb386697",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    '''Multi head attention Module'''\n",
    "    \n",
    "    def __init__(self, n_heads=8, d_model=512, d_k=64, d_v=64, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_heads=n_heads\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        \n",
    "        # initalize the query, key and value matrices\n",
    "        self.w_q = nn.Linear(d_model, n_heads*d_k, bias=False)\n",
    "        self.w_k = nn.Linear(d_model, n_heads*d_k, bias=False)\n",
    "        self.w_v = nn.Linear(d_model, n_heads*d_v, bias=False)\n",
    "        \n",
    "        # scaled dot product attention\n",
    "        self.attention = ScaledDotProductAttention(scaling=d_k ** 0.5)\n",
    "        \n",
    "        # initalize the matrix that multiplies with concatenated outputs\n",
    "        self.fc = nn.Linear(n_heads*d_v, d_model, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # q, k, v are of shape bs X words X d_model\n",
    "        \n",
    "        batch_size = q.size[0]\n",
    "        n_query = q.size[1]\n",
    "        n_keys = k.size[1]\n",
    "        n_values = v.size[1]\n",
    "        \n",
    "\n",
    "        residual_q = q\n",
    "        \n",
    "        # view differnt heads separately\n",
    "        q = self.w_q(q).view(batch_size, n_query, self.n_heads, self.d_k)\n",
    "        k = self.w_k(k).view(batch_size, n_key, self.n_heads, self.d_k)\n",
    "        v = self.w_v(v).view(batch_size, n_value, self.n_heads, self.d_v)\n",
    "        \n",
    "        # Now transpose to bring in format batch X n_heads X words X d\n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "        \n",
    "        # z has dimension batch X n_heads X words X d_v\n",
    "        z, attn = self.attention(q, k, v, mask=mask)\n",
    "        \n",
    "        '''\n",
    "        concatenate all attention outputs in Z, \n",
    "        make new dimesion as batch X words X (n_heads*d_v),\n",
    "        then pass through fc layer and make dim batch X words X d_model\n",
    "        \n",
    "        ''' \n",
    "        # try replacing n_query with n_keys\n",
    "        z = self.fc(z.tranpose(1,2).contguous().view(batch_size, n_query, -1))\n",
    "        \n",
    "        # residual connection\n",
    "        z += residual_q\n",
    "        \n",
    "        z = self.layer_norm(z)\n",
    "        \n",
    "        return z, attn    \n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c6b358d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    '''A two layer feed forward NN '''\n",
    "    \n",
    "    def __init__(self, input_dim=512, hidden_dim=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # position wise\n",
    "        self.w_1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.w_2 = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(input_dim, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x += self.w_2(F.relu(self.w_1(x)))\n",
    "        \n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff97b9e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1373fe47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eb06ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
